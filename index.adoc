= Where no CPU has gone before
:imagesdir: images
:icons: font
:date: September 13, 2018
:my_name: Peter Steinbach
:my_email: steinbach@scionics.de
:stem:

http://dcms.tu-dresden.de/project/dcms-materials-4-0-summer-school-2018/[DCMS Materials 4.0 Summer School 2018] +

mailto:{my_email}[{my_name}], {date}, Dresden, Germany

== Preface

=== My employer

image::scionics_main_logo.png[width=80%]

https://scionics.de[Scionics Computer Innovation GmbH]

[NOTE.speaker]
--
- offer scientific consulting 
- data analysis, large data handling, ...
--

=== Our client

image::800px-MPI-CBG_building_outside_4pl.jpg[]

https://mpi-cbg.de[Max Planck Institute for Molecular Cell Biology and Genetics]

[NOTE.speaker]
--
- scientific computing facility
- my role
--


=== Disclaimer

[.stretch]
image::bart_simpson_white.png[]

These slides are open-source:

https://github.com/psteinb/materials4.0-2018[github.com/psteinb/materials4.0-2018]

== Deep Learning in bits and pieces

image::Typical_cnn.png[]


=== Heavy-Lifting inside CNNs

[cols="^.<,^.<",width=100%,frame=none,grid=none] 
|===
a| image:3D_Convolution_Animation.gif[width=100%]
a| image:Matrix_multiplication_diagram_2.png[width=100%]

s| Convolutions
s| Matrix Operations
|===

=== A closer look

- Convolutions +
  latexmath:[y_i = \sum_{n = 0}^{N_k} x_{i+/-n}*k_{i+/-n} ]

- Matrix Operations +
  latexmath:[AB=Y, y_{ij} = \sum_{k} a_{ik} * b_{kj} ]

- Common? +
**Dot Product Structure!**

[NOTE.speaker]
--
- thousands of dot-products
- one HD frame with 3x3 kernel:
 2.067.604 independent pixels
35.149.268 flops
37.216.872 loads
 2.067.604 stores
--

=== Where do CPUs come from ?

image::wing-commander.jpg[width=100%]

Low Latency Matters Most

[NOTE.speaker]
--
- PC users don't want to wait!
--

=== GPUs for Deep Learning 1/2

image::gpu_cpu_dichotomy.svg[width=100%]

[NOTE.speaker]
--
- GPU: smallest unit of concurrency 32 (>3000 cores)
- CPU: smallest unit of concurrency 1 (10-20 cores)
--

=== GPUs for Deep Learning 2/2

image::high_throughput_smx.svg[width=100%]

Latency Hiding

[NOTE.speaker]
--
- GPU: hides latency of memory access (larger bandwidth)
- CPU: can hide latency to some degree only
--

=== The rest is https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/[history]

image::image_classification_006_x600.png[height=100%]

[NOTE.speaker]
--
- 2012 first deep learning net by Alex Krizhevsky et al
--

=== Consequences on the market

image::nvidia_stock.png[]

Nvidia's stock pricing in the last years

== Benchmarks

image::directions.png[]

[NOTE.speaker]
--
- beginners typically don't know where to go
- which framework?
- web is full of good advice
--

=== Benchmarks ... What for?

- *Executive* to decide what to buy new hardware
- *Developer* to compare framework performance
- *User* wanting to e.g. classify images (fast/slow?)

[NOTE.speaker]
--
- who are they for?
--

=== fair benchmarks

- (deep learning) applications try to solve a problem
- model written in a particular software framework
- running on particular hardware for training and/or inference

TIP: Fix at least 2 of 3 from above!

=== https://dawn.cs.stanford.edu/benchmark/[DawnBench]

- open-source and community driven
- key requirement: reach fixed accuracy for training for fixed dataset

WARNING: data from 1 run only

WARNING: submitter can choose model implementation

WARNING: data inconsistent (K80 cloud-only, P100 bare-only)

=== https://mlperf.org/[MLperf]

- open-source and community driven
- industry support (AMD, Google, Intel, ...)
- goal: SPEC benchmark for Deep Learning

WARNING: data = best of 5 runs

=== https://github.com/psteinb/deeprace[deeprace]

- usable benchmark with clear https://semver.org[semver] support
- model code is fixed
- **ResNet** with Keras+TensorFlow or just TensorFlow (code adopted from official repos)
- single and multi-gpu training (distributed planned)
- data will be open-sourced once I find a sponsor

== Deeprace Results

=== Hardware

* *local cluster*: https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/SystemTaurus[Taurus] at Technical University Dresden
** single GPU node:
*** Intel Xeon E5-2680 v3 12c
*** 64GB RAM
*** 4x Nvidia Tesla K80 GPU
* local servers (Nvidia Titan Xp, Nvidia Tesla P100)

=== Using ResNet on CIFAR10

image::deeprace-full-single.svg[]

[NOTE.speaker]
--
- Resnet32v1 (and Resnet56v1) as sample models on CIFAR10 dataset
- time-per-epoch higher for smaller batches (more host-device transfers, backprop more often)
--

=== Containers!

image::deeprace-full-vs-singularity.png[width=100%]

https://www.sylabs.io/docs/[singularity] container = https://keras.io[Keras 2.1.5] + https://tensorflow.org[TensorFlow 1.3.0]       


[NOTE.speaker]
--
- for setup and reproducibility
- for the rest, use tf 1.7
--

=== Short runs only

image::deeprace-short-runtimes.png[]

[NOTE.speaker]
--
- as time per epoch is "flat" -> limit to `n=15` epochs
- multiple runs per measurements
--

=== single-GPU training

image::deeprace-short-hw.png[]

[NOTE.speaker]
--
- architecture difference Pascal (2016) and Kepler (2013/2014)
- note: gaming GPUs
--

=== cloud?

image::deeprace-short-runtimes-vs-cloud.svg[]

GCE, single K80 instance, 1vCPU, 6GB RAM, 10GB disk

[NOTE.speaker]
--
- keras:2.1.5,tensorflow:1.7.0 
--

=== framework differences?

image::deeprace-frameworks.svg[]

[NOTE.speaker]
--
- Titan Xp
--

=== multi-GPU training

image::deeprace-short-multi-gpu-compared.png[]

[NOTE.speaker]
--
- keras:2.1.5,tensorflow:1.7.0 
- batch based parallelisation typically with gradient averaging (only possible with tensorflow 1.5+)
- no linear scaling is a https://medium.com/rossum/towards-efficient-multi-gpu-training-in-keras-with-tensorflow-8a0091074fb2[known problem]
- numa at play for 2GPU use case
- still a lot TODOs
--

== Summary

- deep learning requires a lot of parallel compute power
- GPUs et al are indispensible tools
- hardware/framework landscape diverse
- solid benchmarks save time & money

== Where No Hardware can go 

.https://arxiv.org/pdf/1803.09820.pdf[Super Convergence, arxiv:1803.09820]
image:super-convergence.png[]

[NOTE.speaker]
--
* heuristic to speed up training
* loss function landscape "known"
* Cyclical learning rates: specify min. and max. learning rate boundaries
* Specify number of cycle iterations
* In each cycle increase learning rate linearly from  min to max and then back down
* Do a few cycles
* bottom line: better algorithms always win!
--
